{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlBCDDZsPqJPwRG3v3chLR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidetassinari/web-scraper-analyzer/blob/main/20250223_Async_scraper_Good_enough_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of contents\n",
        "\n",
        "1. Introduction\n",
        "1. Mount Your Google Drive\n",
        "1. Load the Input Website List in `.csv` Form\n",
        "1. Perform a Simple Crawl of the Websites in the List\n",
        "1. Scrape Plain Text from the List of Crawled Webpages for Each Site\n",
        "1. Analyse Website Text Using OpenAI's Async API\n",
        "1. Clean-up and Parsing of LLM Output\n",
        "1. Organise All Information in a Spreadsheet"
      ],
      "metadata": {
        "id": "avNq4vDDW4zE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Introduction\n",
        "\n",
        "Install any needed dependencies."
      ],
      "metadata": {
        "id": "ZFWrMhsPsWuo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "58FgKa0YVy_f"
      },
      "outputs": [],
      "source": [
        "!pip install openai requests pandas beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Mount Your Google Drive\n",
        "\n",
        "Mount your Drive in order to access your `.csv` file.\n",
        "\n",
        "You need to do this every time you open this notebook or reconnect to an environment.\n",
        "\n",
        "Run the next cell to authorise access:\n",
        "- Select \"Connect to Google Drive\" in the small popup\n",
        "- Choose the correct account from the list in the popup window\n",
        "- Sign in to that account\n",
        "- The first time you authorise accesso to Drive you will need to select all permissions. After that you can just select \"Continue\" unless you choose to manually review the permissions.\n",
        "\n",
        "After you run this code block you should see the message `Mounted at /content/drive` at the bottom."
      ],
      "metadata": {
        "id": "u_Swcvz7XmXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dluMm89OX9PF",
        "outputId": "e5289f07-5d69-4c64-ab1c-696e3b1d3983"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load the Input Website List in `.csv` Form\n",
        "\n",
        "Place your `.csv` file (e.g., `venture_websites.csv`) in a known folder on your\n",
        "Drive.\n",
        "\n",
        "Note the file path, as you will use it to load your data.\n",
        "\n",
        "The `.csv` file must have a header row with the label \"Website\"."
      ],
      "metadata": {
        "id": "HH7gS1-VYLLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace the file path below with the actual path to your CSV file in Drive\n",
        "csv_file_path = '/content/drive/MyDrive/Coding/VentureBuilders/0_website_csv/0_venture_websites.csv'\n",
        "\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "print(\"CSV loaded. Number of records:\", len(df))"
      ],
      "metadata": {
        "id": "cxmGLSxFYQ_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Perform a Simple Crawl of the Websites in the List\n",
        "\n",
        "The following code will loop through each URL in your `.csv` file, performing a simple two-level deep crawl and deleting duplicates.\n",
        "\n",
        "Websites that are unreachable or return errors are simply skipped.\n",
        "\n",
        "The results are saved into a new `JSON` file in a specified folder."
      ],
      "metadata": {
        "id": "KEaH8BrUahCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import pandas as pd # Might be legacy from when output was saved as csv\n",
        "import concurrent.futures\n",
        "import json\n",
        "\n",
        "pages_to_scrape = '/content/drive/MyDrive/Coding/VentureBuilders/1_web_page_lists/1_website_crawl.json'\n",
        "\n",
        "def get_links_from_page(url):\n",
        "    \"\"\"Scrape all internal links from a given webpage.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        base_domain = urlparse(url).netloc\n",
        "\n",
        "        links = set()\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            absolute_url = urljoin(url, href)\n",
        "            if urlparse(absolute_url).netloc == base_domain:\n",
        "                links.add(absolute_url)\n",
        "\n",
        "        return links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to retrieve {url}: {e}\")\n",
        "        return set()\n",
        "\n",
        "\n",
        "def normalize_url(url):\n",
        "    \"\"\"Normalize URL by removing fragments and trailing slashes.\"\"\"\n",
        "    parsed = urlparse(url)._replace(fragment='')\n",
        "    normalized_url = parsed.geturl().rstrip('/')\n",
        "    return normalized_url\n",
        "\n",
        "\n",
        "def check_url(url):\n",
        "    \"\"\"Check if a URL is reachable.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"URL check failed for {url}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def scrape_website_links(input_csv, output_json, max_depth=2):\n",
        "    \"\"\"Scrapes websites to a given depth and writes to a JSON file.\"\"\"\n",
        "    websites = pd.read_csv(input_csv)\n",
        "    result_data = []\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        for _, row in websites.iterrows():\n",
        "            base_url = normalize_url(row['Website'])\n",
        "\n",
        "            # Check if the base URL is reachable\n",
        "            if not check_url(base_url):\n",
        "                print(f\"Skipping unreachable URL: {base_url}\")\n",
        "                continue\n",
        "\n",
        "            visited = set([base_url])\n",
        "            all_pages = {base_url}\n",
        "            future_to_url = {executor.submit(get_links_from_page, base_url): (base_url, 1)}\n",
        "\n",
        "            while future_to_url:\n",
        "                completed, _ = concurrent.futures.wait(future_to_url.keys(), return_when=concurrent.futures.FIRST_COMPLETED)\n",
        "                for future in completed:\n",
        "                    url, depth = future_to_url.pop(future)\n",
        "                    try:\n",
        "                        links = future.result()\n",
        "                        new_links = {normalize_url(link) for link in links if normalize_url(link) not in visited}\n",
        "                        all_pages.update(new_links)\n",
        "                        visited.update(new_links)\n",
        "\n",
        "                        if depth < max_depth:\n",
        "                            for link in new_links:\n",
        "                                future_to_url[executor.submit(get_links_from_page, link)] = (link, depth + 1)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {url}: {e}\")\n",
        "\n",
        "            result_data.append({\n",
        "                \"base_website\": base_url,\n",
        "                \"pages\": list(all_pages)\n",
        "            })\n",
        "\n",
        "    # Save to JSON file\n",
        "    with open(output_json, 'w') as f:\n",
        "        json.dump(result_data, f, indent=4)\n",
        "    print(f\"Scrape completed and saved to {output_json}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "scrape_website_links(csv_file_path, pages_to_scrape)"
      ],
      "metadata": {
        "id": "fFcnBqjl4VVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Scrape Plain Text from the List of Crawled Webpages for Each Site\n",
        "\n",
        "Once all existing websites have been crawled, all the pages are visited sequentially and scraped for plain text. `PDF` files are skipped.\n",
        "\n",
        "Text length is limited to `10000` characters for each page to avoid feeding excessively large files to the LLM in the following step.\n",
        "\n",
        "One output file is created for each website in the original list, with a simple `JSON` schema. The filename is base on the original URL in the list.\n",
        "\n",
        "The original URL is preserved in each of the following steps for compatibility with the initial website list."
      ],
      "metadata": {
        "id": "DDE2WwTeF3Xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Paths and directories\n",
        "input_path = pages_to_scrape\n",
        "# input_path = '/content/drive/MyDrive/Coding/VentureBuilders/1_web_page_lists/1_website_crawl.json'\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/Coding/VentureBuilders/2_full_site_scrapes'\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load the intermediate JSON file with base URLs\n",
        "with open(input_path, 'r') as f:\n",
        "    base_urls = json.load(f)\n",
        "\n",
        "def scrape_page_content(url):\n",
        "    \"\"\"Scrape text content from a single webpage.\"\"\"\n",
        "    try:\n",
        "        if url.lower().endswith('.pdf'):\n",
        "            return {\"url\": url, \"error\": \"Skipped PDF file\"}\n",
        "\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code != 200:\n",
        "            return {\"url\": url, \"error\": f\"HTTP {response.status_code}\"}\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        title = soup.title.string if soup.title else \"No title\"\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"title\": title,\n",
        "            \"content\": text[:10000]  # limit text length to avoid excessive data\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"url\": url, \"error\": str(e)}\n",
        "\n",
        "def sanitize_filename(url):\n",
        "    \"\"\"Create a legible filename by replacing URL dots with underscores.\"\"\"\n",
        "    return url.replace('https://', '').replace('http://', '').replace('www.', '').replace('/', '_').replace('.', '_')\n",
        "\n",
        "def is_url_accessible(url):\n",
        "    \"\"\"Check if the base URL is accessible.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        return response.status_code == 200\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to connect to {url}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Iterate over each base URL and its pages\n",
        "for entry in base_urls:\n",
        "    base_url = entry['base_website']\n",
        "    site_name = sanitize_filename(base_url)\n",
        "    print(f\"Checking site: {base_url}\")\n",
        "\n",
        "    # Check if the site is accessible\n",
        "    if not is_url_accessible(base_url):\n",
        "        print(f\"Skipping site: {base_url} due to connection issues\")\n",
        "        continue\n",
        "\n",
        "    site_content = [{\n",
        "        \"original_url\": base_url,\n",
        "        \"sanitized_name\": site_name\n",
        "    }]\n",
        "\n",
        "    for page_url in entry['pages']:\n",
        "        if page_url.lower().endswith('.pdf'):\n",
        "            print(f\"Skipping PDF file: {page_url}\")\n",
        "            continue\n",
        "        print(f\"Scraping page: {page_url}\")\n",
        "        page_data = scrape_page_content(page_url)\n",
        "        site_content.append(page_data)\n",
        "\n",
        "    # Save the scraped content to a JSON file\n",
        "    output_file = os.path.join(output_dir, f\"scraped_{site_name}.json\")\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(site_content, f, indent=4)\n",
        "\n",
        "    print(f\"Finished scraping {base_url}. Data saved to: {output_file}\")\n",
        "\n",
        "print(\"Scraping complete for all sites.\")"
      ],
      "metadata": {
        "id": "sMbvPhjKGAz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Analyse Website Text Using OpenAI's Async API\n",
        "\n",
        "Once all scraped text files have been created in the chosen folder, each file is sent for analysis to OpenAI's Async API.\n",
        "\n",
        "The chosen model is currently (as of 2025-02-23) `GPT-4o-mini` since:\n",
        "\n",
        "- It is adequate for simple text analysis and output\n",
        "- It allows for the largest async requests (200k tokens)\n",
        "- It has the largest number of async requests (500 requests per minute)\n",
        "- It has the largest daily limit (2 M tokens per day)\n",
        "\n",
        "The API key is stored as a Colab Secret as `OPENAI_API_KEY`.\n",
        "\n",
        "The prompt needs to be customised to perform the required analysis.\n",
        "\n",
        "The prompt also specifies the `JSON` schema of the output as a quick and dirty stand-in for a structured output. The following code block performs some cleanup and some local parsing to ensure consistency in `JSON` keys.\n",
        "\n",
        "For each input scraped text `JSON` file in the folder, an output analysis `JSON` file is created in the specified folder. File names are maintained consistent with a prefix that clarifies their stage for clarity.\n",
        "\n",
        "Async calls allow for some flexibility when the number of tokens per minute is close to the limit."
      ],
      "metadata": {
        "id": "UfBCdJeGKvBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "from openai import AsyncOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Paths and directories\n",
        "# output_dir = '/content/drive/MyDrive/Coding/VentureBuilders/Full_Site_Scrapes'\n",
        "output_dir = '/content/drive/MyDrive/Coding/VentureBuilders/2_full_site_scrapes' # output_dir repeated so I can only run this code block\n",
        "analysis_dir = '/content/drive/MyDrive/Coding/VentureBuilders/3_analysis_results'\n",
        "os.makedirs(analysis_dir, exist_ok=True)\n",
        "\n",
        "# Initialize OpenAI client\n",
        "\n",
        "API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "# print(API_KEY)\n",
        "client = AsyncOpenAI(api_key=API_KEY)\n",
        "\n",
        "async def analyze_site_content(file_path, output_file_path):\n",
        "    \"\"\"Analyze the content of a site using the OpenAI API.\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        site_data = json.load(f)\n",
        "\n",
        "    # Initialize original_url and determine where the page data is stored.\n",
        "    original_url = None\n",
        "    if isinstance(site_data, dict) and \"original_url\" in site_data:\n",
        "        original_url = site_data[\"original_url\"]\n",
        "        pages = site_data.get(\"pages\", [])\n",
        "    elif isinstance(site_data, list) and len(site_data) > 0 and isinstance(site_data[0], dict) and \"original_url\" in site_data[0]:\n",
        "        original_url = site_data[0][\"original_url\"]\n",
        "        pages = site_data[1:]  # assume the first element is metadata and the rest are pages\n",
        "    else:\n",
        "        pages = site_data  # assume site_data is a list of pages if no original_url is present\n",
        "\n",
        "    # Construct a text block from the pages for the prompt.\n",
        "    content_text = \"\\n\\n\".join(\n",
        "        f\"Title: {page.get('title', 'No title')}\\nContent: {page.get('content', '')[:3000]}\" # Not sure this 3000 limit makes sense. Tokens? Characters?\n",
        "        for page in pages\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        \"You are a skilled MBA graduate with experience in the startup and venture building world. \"\n",
        "        \"Analyse the following information and summarise it based on the following attributes:\\n\\n\"\n",
        "        \"## Venture Builder Name\\n\"\n",
        "        \"The name of the Venture Builder organization.\\n\\n\"\n",
        "        \"## Guild\\n\"\n",
        "        \"The Guild is the infrastructure package that Venture builders supply to their startup companies.\\n\"\n",
        "        \"It includes the entrepreneurs-in-residence who will form the backbone of the new ventures, technological know-how, financing, and physical spaces.\\n\"\n",
        "        \"The Guild may be shared horizontally between different startups in the portfolio to increase cost-effectiveness, or it can be distributed vertically for each business.\\n\\n\"\n",
        "        \"## Control\\n\"\n",
        "        \"Venture builders have different rules governing the equity they hold in startups, according to their coaching intensity and required degree of control.\\n\\n\"\n",
        "        \"## Focus\\n\"\n",
        "        \"Venture builders can choose to operate across different sectors, covering a wide spectrum of technologies and markets, or they may operate vertically in a specific sector or on a specific technological solution (e.g. Artificial Intelligence or Deep Tech). \"\n",
        "        \"The first approach is called the Generalist while the second is called Specialist.\\n\\n\"\n",
        "        \"## Idea\\n\"\n",
        "        \"Idea generation is a critical component of the Business Model of venture builders. Sourcing might be done externally or internally. \"\n",
        "        \"External sourcing may take two forms: fostering external early-stage firms or scanning the market for the best venture to produce a Copycat.\\n\\n\"\n",
        "        \"## Volume\\n\"\n",
        "        \"Startup studios can develop a few or many businesses concurrently.\\n\\n\"\n",
        "        \"## Funding\\n\"\n",
        "        \"Startup studios can choose to finance their portfolio with their own resources or can choose external funding. \"\n",
        "        \"Hybrid solutions are common, with external partners piggybacking onto the funding that the Venture-Builder makes available for the start-ups.\\n\\n\"\n",
        "        \"## Scaling\\n\"\n",
        "        \"Startup studios are concerned with the timing and strategy of their exit with regards to scaling. Relevant terminology includes, but is not limited to, \"\n",
        "        \"scaling, scalers, scaleup, unicors and gazelles. The main focus of this section is on at what time in the startup life the startup studio aims to exit.\\n\"\n",
        "        \"## Final remarks\\n\"\n",
        "        \"Any final conclusions.\\n\\n\"\n",
        "        \"Instructions:\\n\"\n",
        "        \"* Your analysis should be thorough, comprehensive and objective.\\n\"\n",
        "        \"* Rely exclusively on information found on the official website of the venture builder.\\n\"\n",
        "        \"* Avoid assumptions or external sources.\\n\"\n",
        "        \"* If any attribute information is not available on the website, indicate it as not disclosed or not available, by simply using \\\"Not disclosed\\\" or \\\"Not available\\\" without additional detail.\\n\\n\"\n",
        "        \"* The output should be formatted according to JSON standards for ease of parsing, with the category as a key and the analysis content as the value.\\n\"\n",
        "        \"* Each JSON value for the main sections should be in natural language format for ease of consultation, and not contain any nested dicts or key-value pairs.\\n\\n\"\n",
        "        \"* The size of the output value for each category and each venture builder should be commensurate to the amount of information retrieved and may be lengthy if required.\\n\"\n",
        "        \"The text to be analyzed follows.\\n\\n\\n\" + content_text\n",
        "    )\n",
        "\n",
        "    response = await client.chat.completions.create(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        model=\"gpt-4o-mini\",\n",
        "    )\n",
        "\n",
        "    analysis = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Build output JSON with the preserved original_url (if available) at the beginning.\n",
        "    output_data = {}\n",
        "    if original_url is not None:\n",
        "        output_data[\"original_url\"] = original_url\n",
        "    output_data[\"analysis\"] = analysis\n",
        "\n",
        "    # Save analysis to the provided output file path.\n",
        "    with open(output_file_path, 'w') as f:\n",
        "        json.dump(output_data, f, indent=4)\n",
        "\n",
        "    print(f\"Analysis saved to {output_file_path}\")\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Run analyses for all JSON files in the output directory.\"\"\"\n",
        "    tasks = []\n",
        "    output_dir_real = os.path.realpath(output_dir)\n",
        "    for file_name in os.listdir(output_dir):\n",
        "        file_path = os.path.join(output_dir, file_name)\n",
        "        if os.path.isdir(file_path):\n",
        "            continue\n",
        "        if os.path.realpath(os.path.dirname(file_path)) != output_dir_real:\n",
        "            continue\n",
        "        if file_name.endswith('.json'):\n",
        "            # Create new filename by replacing \"scraped_\" with \"analyzed_\"\n",
        "            if file_name.startswith(\"scraped_\"):\n",
        "                analyzed_file_name = \"analyzed_\" + file_name[len(\"scraped_\"):]\n",
        "            else:\n",
        "                analyzed_file_name = \"analyzed_\" + file_name\n",
        "            output_file_path = os.path.join(analysis_dir, analyzed_file_name)\n",
        "            tasks.append(analyze_site_content(file_path, output_file_path))\n",
        "    await asyncio.gather(*tasks)\n",
        "\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "asyncio.run(main())\n",
        "print(\"Batch analysis complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pqNUCWYK1Xu",
        "outputId": "3257face-4942-4e0b-c1be-7647612946ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis saved to /content/drive/MyDrive/Coding/VentureBuilders/3_analysis_results/analyzed_sap_io.json\n",
            "Batch analysis complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Clean-up and Parsing of LLM Output\n",
        "\n",
        "Since LLM outputs do not strictly adhere to a `JSON` schema (unless structured output is implemented), a cleanup and parsing step is required.\n",
        "\n",
        "Any further internal `JSON` structure in the analysis values (further lists or dicts) is flattened to improve legibility in the final spreadsheet.\n",
        "\n",
        "Again, for each file in the analysis result folder a new parsed file is created in the specified folder and the URL from the initial `.csv` is preserved."
      ],
      "metadata": {
        "id": "JbhSGojr631q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Paths and directories\n",
        "analysis_dir = '/content/drive/MyDrive/Coding/VentureBuilders/3_analysis_results'\n",
        "parsed_dir = '/content/drive/MyDrive/Coding/VentureBuilders/4_parsed_results'\n",
        "os.makedirs(parsed_dir, exist_ok=True)\n",
        "\n",
        "def normalize_key(key):\n",
        "    \"\"\"Convert a key to lowercase and remove spaces, dashes, and underscores.\"\"\"\n",
        "    return key.lower().replace(' ', '').replace('-', '').replace('_', '')\n",
        "\n",
        "def parse_analysis_file(file_path):\n",
        "    \"\"\"Parse the analysis text into structured JSON with flattened natural language output,\n",
        "       preserving the original URL and normalizing keys.\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Capture the original URL from the input JSON\n",
        "    original_url = data.get('original_url', '')\n",
        "\n",
        "    analysis_text = data.get('analysis', '')\n",
        "\n",
        "    # Extract JSON content from the code block\n",
        "    json_match = re.search(r'```json\\n(.*?)\\n```', analysis_text, re.DOTALL)\n",
        "    if json_match:\n",
        "        clean_json = json_match.group(1)\n",
        "        try:\n",
        "            parsed_data = json.loads(clean_json)\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Failed to parse JSON for {file_path}\")\n",
        "            parsed_data = {}\n",
        "    else:\n",
        "        print(f\"No valid JSON found in {file_path}\")\n",
        "        parsed_data = {}\n",
        "\n",
        "    # Flatten the structure into plain text paragraphs with \"Key: value.\" formatting\n",
        "    flattened_data = {}\n",
        "    for key, value in parsed_data.items():\n",
        "        norm_key = normalize_key(key)\n",
        "        if isinstance(value, dict):\n",
        "            # Build a string for each normalized subkey/value pair in the dict\n",
        "            inner_text = \". \".join(f\"{normalize_key(subkey)}: {subvalue}\" for subkey, subvalue in value.items())\n",
        "            flattened_data[norm_key] = inner_text\n",
        "        elif isinstance(value, list):\n",
        "            # Join list items as comma-separated text\n",
        "            list_text = \", \".join(str(item) for item in value)\n",
        "            flattened_data[norm_key] = list_text\n",
        "        else:\n",
        "            flattened_data[norm_key] = value\n",
        "\n",
        "    # Remove any square brackets that might appear in the text\n",
        "    flattened_data = {k: v.replace(\"[\", \"\").replace(\"]\", \"\") for k, v in flattened_data.items()}\n",
        "\n",
        "    # Add the original URL with normalized key\n",
        "    flattened_data[\"originalurl\"] = original_url\n",
        "\n",
        "    # Determine the output file name by replacing \"analyzed_\" with \"parsed_\"\n",
        "    file_basename = os.path.basename(file_path)\n",
        "    if file_basename.startswith(\"analyzed_\"):\n",
        "        parsed_file_name = \"parsed_\" + file_basename[len(\"analyzed_\"):]\n",
        "    else:\n",
        "        parsed_file_name = \"parsed_\" + file_basename\n",
        "\n",
        "    # Save the flattened data to a new JSON file in parsed_dir\n",
        "    output_file = os.path.join(parsed_dir, parsed_file_name)\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(flattened_data, f, indent=4)\n",
        "\n",
        "    print(f\"Parsed analysis saved to {output_file}\")\n",
        "\n",
        "# Process all analysis files\n",
        "for file_name in os.listdir(analysis_dir):\n",
        "    if file_name.endswith('.json'):\n",
        "        file_path = os.path.join(analysis_dir, file_name)\n",
        "        parse_analysis_file(file_path)\n",
        "\n",
        "print(\"Parsing complete for all analysis files.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OYZCi4CLtzL",
        "outputId": "4ace37b8-5e66-4ed2-9bc8-3981b56a4932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed analysis saved to /content/drive/MyDrive/Coding/VentureBuilders/4_parsed_results/parsed_sap_io.json\n",
            "Parsing complete for all analysis files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Organise All Information in a Spreadsheet\n",
        "\n",
        "The parsed analysis output is finally inserted in a Google Sheets spreadsheet.\n",
        "\n",
        "Column headers correspond to the analysis requested in the LLM prompt."
      ],
      "metadata": {
        "id": "rxGxm9j178Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Authenticate and initialize gspread\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# Paths and directories\n",
        "parsed_dir = '/content/drive/MyDrive/Coding/VentureBuilders/4_parsed_results'\n",
        "sheet_name = 'venture_builders_analysis'\n",
        "project_root_name = 'VentureBuilders'  # This folder corresponds to '/content/drive/MyDrive/Coding/VentureBuilders/'\n",
        "\n",
        "# Open or create the Google Sheet\n",
        "try:\n",
        "    sh = gc.open(sheet_name)\n",
        "except gspread.exceptions.SpreadsheetNotFound:\n",
        "    sh = gc.create(sheet_name)\n",
        "\n",
        "# Use Google Drive API to move the spreadsheet into the project root folder\n",
        "drive_service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "# Search for the folder \"VentureBuilders\" in MyDrive\n",
        "results = drive_service.files().list(\n",
        "    q=f\"mimeType='application/vnd.google-apps.folder' and name='{project_root_name}' and trashed=false\",\n",
        "    fields=\"files(id, name)\"\n",
        ").execute()\n",
        "items = results.get('files', [])\n",
        "if items:\n",
        "    project_folder_id = items[0]['id']\n",
        "else:\n",
        "    # Create the folder if it doesn't exist\n",
        "    file_metadata = {\n",
        "        'name': project_root_name,\n",
        "        'mimeType': 'application/vnd.google-apps.folder'\n",
        "    }\n",
        "    folder = drive_service.files().create(body=file_metadata, fields='id').execute()\n",
        "    project_folder_id = folder.get('id')\n",
        "\n",
        "# Move the spreadsheet to the project folder\n",
        "file_id = sh.id\n",
        "file_info = drive_service.files().get(fileId=file_id, fields='parents').execute()\n",
        "previous_parents = \",\".join(file_info.get('parents'))\n",
        "drive_service.files().update(\n",
        "    fileId=file_id,\n",
        "    addParents=project_folder_id,\n",
        "    removeParents=previous_parents,\n",
        "    fields='id, parents'\n",
        ").execute()\n",
        "\n",
        "# Select the \"Analysis\" worksheet or create it if it doesn't exist\n",
        "try:\n",
        "    worksheet = sh.worksheet('Analysis')\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    worksheet = sh.add_worksheet(title='Analysis', rows='1000', cols='10')\n",
        "\n",
        "# Define headers for the data\n",
        "headers = [\n",
        "    \"Original URL\", \"Venture Builder Name\", \"Guild\", \"Control\",\n",
        "    \"Focus\", \"Idea\", \"Volume\", \"Funding\", \"Scaling\", \"Final remarks\"\n",
        "]\n",
        "\n",
        "# Ensure the header row is present by updating cells A1:J1 using named arguments\n",
        "worksheet.update(values=[headers], range_name='A1:J1')\n",
        "\n",
        "# Read JSON files from parsed_dir and append each as a new row\n",
        "for file_name in os.listdir(parsed_dir):\n",
        "    if file_name.endswith('.json'):\n",
        "        file_path = os.path.join(parsed_dir, file_name)\n",
        "        with open(file_path, 'r') as f:\n",
        "            parsed_data = json.load(f)\n",
        "\n",
        "        row = [\n",
        "            parsed_data.get(\"originalurl\", \"\"),\n",
        "            parsed_data.get(\"venturebuildername\", \"\"),\n",
        "            parsed_data.get(\"guild\", \"\"),\n",
        "            parsed_data.get(\"control\", \"\"),\n",
        "            parsed_data.get(\"focus\", \"\"),\n",
        "            parsed_data.get(\"idea\", \"\"),\n",
        "            parsed_data.get(\"volume\", \"\"),\n",
        "            parsed_data.get(\"funding\", \"\"),\n",
        "            parsed_data.get(\"scaling\", \"\"),\n",
        "            parsed_data.get(\"finalremarks\", \"\")\n",
        "        ]\n",
        "        worksheet.append_row(row, value_input_option='USER_ENTERED')\n",
        "        print(f\"Analysis appended for {file_name}\")\n",
        "\n",
        "print(\"All parsed analysis files have been appended into the Google Sheet.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM3weQ4DX68C",
        "outputId": "73f768c8-2063-4aec-ecf5-cd98e7891d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis appended for parsed_sap_io.json\n",
            "All parsed analysis files have been appended into the Google Sheet.\n"
          ]
        }
      ]
    }
  ]
}